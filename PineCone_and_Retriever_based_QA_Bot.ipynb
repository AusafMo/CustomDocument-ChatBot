{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31bde098c973405e96d3f510de787dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dac7cf8902049eba8c35964ca7d8c64",
              "IPY_MODEL_339c39d6590643bdaea1022580fb9c23",
              "IPY_MODEL_9b389355cbbe45e7947920c8462fb39a"
            ],
            "layout": "IPY_MODEL_132cdd933b0847749e11c909904efbdd"
          }
        },
        "0dac7cf8902049eba8c35964ca7d8c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05a0a0d7cbc4837b6f387ff872ea81d",
            "placeholder": "​",
            "style": "IPY_MODEL_08b5fbe958d643e081ad42d2d2e08006",
            "value": "Downloading (…)b-python.Q5_K_M.gguf: 100%"
          }
        },
        "339c39d6590643bdaea1022580fb9c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c44fe1516e747b0ba68eeb862de8523",
            "max": 9229924288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8524be071c484f0fb223cb642c890766",
            "value": 9229924288
          }
        },
        "9b389355cbbe45e7947920c8462fb39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbb310ddb8844e9f8374c07b9561df8c",
            "placeholder": "​",
            "style": "IPY_MODEL_8c3c9e002ebd4a82ab75f8307d06c02e",
            "value": " 9.23G/9.23G [08:06&lt;00:00, 18.0MB/s]"
          }
        },
        "132cdd933b0847749e11c909904efbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05a0a0d7cbc4837b6f387ff872ea81d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b5fbe958d643e081ad42d2d2e08006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c44fe1516e747b0ba68eeb862de8523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8524be071c484f0fb223cb642c890766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbb310ddb8844e9f8374c07b9561df8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3c9e002ebd4a82ab75f8307d06c02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AusafMo/CustomDocument-ChatBot/blob/main/PineCode_and_Retriever_based_QA_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "5qLilW73vIby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QA on a .txt File**"
      ],
      "metadata": {
        "id": "4zN85sHH4k0g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EA88VtbDSRZ",
        "outputId": "49cbc396-18c8-47e4-ab9e-8b5ceddfdccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.329-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.56-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.329 langsmith-0.0.56 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.10.28-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.1)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=203412e2cebea6547ce563f78f0147a8d975ceb47e6e0f1c084d375a7944b1e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, emoji, backoff, unstructured\n",
            "Successfully installed backoff-2.2.1 emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.5.1 unstructured-0.10.28\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=7fe072417ad500a0f217e028ee0d26d50a172d99e10309130e2c6f03dc37268d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.12.tar.gz (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.12-cp310-cp310-manylinux_2_35_x86_64.whl size=1019756 sha256=5b0ae269e37124f7f10f13bd1f8612aca0bed22569533e73ba1bd0a8bbf8d012\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/07/dd/f9549e7045690b8b66dadaa2224a478aa49a61edc8d368a62e\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.12\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "# !pip install pypdf\n",
        "!pip install unstructured\n",
        "!pip install sentence_transformers\n",
        "!pip install pinecone-client\n",
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "a6dX9MMTDSRZ"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACEHUB_API_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_API_ENV = userdata.get('PINECONE_API_ENV')"
      ],
      "metadata": {
        "id": "z4Uok5DgU0CK"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pinecone.delete_index(\"bot\")\n",
        "# pinecone.create_index(\"bot\", dimension=384)\n",
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"bot\" # put in the name of your pinecone index here"
      ],
      "metadata": {
        "id": "lFH2z-8zCejE"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"Idea Description.txt\")\n",
        "data = loader.load()\n",
        "\n",
        "data\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)\n",
        "docs=text_splitter.split_documents(data)\n",
        "len(docs)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATcM0O0e4m5B",
        "outputId": "698b5372-0779-4d1b-f429-d288b9afa028"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Problem Statement: Identification of Different Medicinal Plants/Raw materials through Image Processing Using Machine Learning Algorithms\\n\\n\\nSolutions :', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='A user-friendly Web-based and Android-based platform for users to upload images, find out whether it’s a medical plant/material or not, and provide detailed descriptions of the benefits of the recognized leaf/material using deep learning techniques.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Our Idea/Solution is executed in 4 major phases, viz. Data Collection and Processing, Model Building, Platform Integration and Rigorous Testing. Lets look at them comprehensively :\\n\\nPhase 1: Dataset Collection', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='We have used a publicly available dataset as a starter for the problem, and plan to make this dataset more robust by adding more data points.\\n\\t\\n\\tCurrent Dataset Description:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='The dataset consists of images of 30 species of healthy medicinal herbs, including Santalum album (Sandalwood), Muntingia calabura (Jamaica cherry), Plectranthus amboinicus / Coleus amboinicus (Indian Mint, Mexican mint), Brassica juncea (Oriental', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Brassica juncea (Oriental mustard), and more.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='In total, there are 1,500 images encompassing 30 species, with each species having 60 to 100 high-quality images.\\n\\t\\n\\tDataset: S, Roopashree; J, Anitha (2020), “Medicinal Leaf Dataset”, Mendeley Data, V1, doi: 10.17632/nnytj2v3n5.1', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Phase 2: Building a Model\\n\\tTo solve the problem of identification we employ transfer learning techniques to use state-of-the-art CNN models like ResNet50 and make them fit for our use-case.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Transfer learning is a machine learning technique that leverages the knowledge acquired by a model in one task to improve its performance in another related task. In this case, a pre-trained ResNet-50 model, a deep convolutional neural network known for', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='neural network known for its image classification capabilities, serves as the foundation. This pre-trained model has already learned to recognise and extract valuable features from a vast and diverse dataset.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='We employ two key strategies :', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"1. Feature Extraction: The first strategy is feature extraction. This entails preserving the knowledge and feature extraction abilities of the pre-trained ResNet model. To achieve this, we freeze the model's weights, ensuring that the original features\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='that the original features learned during its extensive training are not overwritten. This is vital because these pre-learned features are valuable for image recognition tasks, including medicinal materials/plant classification.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"2. Custom Classifier: The second strategy involves customising the model for the specific task of medicinal plant/material identification. Instead of using the pre-trained model's original classification layer, a new classifier is added to the model.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='is added to the model. This classifier is tailored to the nuances of the medicinal plant/material classification problem and consists of fully connected layers, activation functions, and dropout layers to prevent overfitting.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='In the training process, we monitoring both training and validation losses and accuracy. We implement early stopping, a technique that halts training when there is no improvement in the validation loss for a predefined number of epochs(rounds). This', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='number of epochs(rounds). This prevents the model from overfitting the training data and ensures that it generalizes well to new, unseen images.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"As the training progresses, we continuously evaluate the model's performance on a separate validation dataset. The best-performing model, based on validation loss, is saved. This means that the model with the lowest validation loss is retained for our\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='loss is retained for our future use.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='The result is a highly accurate model for identifying medicinal plants, with an astounding testing accuracy of 98%. This impressive level of accuracy shows us the effectiveness and potential of the transfer learning approach in solving the challenging', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='in solving the challenging problem of medicinal plant/material identification, ultimately contributing to the trustworthiness and authenticity of the Ayurvedic system.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Phase 2: Creating a platform (we are Continuously Enhancing on our platform)', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Front-End Component:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='The front-end component is the user-facing part of the platform, responsible for interacting with users and facilitating the input of data, particularly images in this context. Users typically access the front end through a user interface, which may be', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='a user interface, which may be a web application, mobile app, or other user-friendly interface. This component collects user inputs, such as images of medicinal plants, and communicates with the back end to process and analyze this data.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Back-End Component:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"Our back-end component, hosted on Google Cloud, plays a significant role in the platform's operation. It is implemented as a Flask application, which is a lightweight and versatile web framework for Python. The back end serves as the engine that\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='end serves as the engine that performs several critical functions:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='1. Input Image Preprocessing: When users submit images of medicinal plants, the back end handles the preprocessing of these images. This preprocessing may involve tasks like resizing, normalizing, and ensuring that the input data is in a suitable format', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='data is in a suitable format for the model. Preprocessing is a crucial step to ensure that the model receives clean and consistent input data.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='2. Inferencing with the Model: The heart of the back-end is the previously described model, which has been fine-tuned and optimized for medicinal plant identification. The back end takes the preprocessed images and passes them through the model for', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='them through the model for inference. The model uses its learned features to classify the medicinal plants accurately. The result of this inference is returned to the front end.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='3. Interaction between components: The back-end component serves as a bridge between the front-end and the machine learning model. It communicates with the front end to receive user inputs and, once the inference is complete, sends the results back to', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='sends the results back to the front end for display to the user.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='4. Scalability and Cloud Hosting: Hosting the back-end on Google Cloud provides scalability, allowing the platform to handle increased user loads efficiently. Google Cloud offers a range of services that can be leveraged for deployment, scaling, and', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='for deployment, scaling, and management, ensuring that the platform is robust and responsive.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"This two-tier architecture neatly separates the user interaction and data collection (front-end) from the model's inference process (back-end), creating a modular and scalable system. It ensures that users can easily and intuitively interact with the\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='intuitively interact with the platform, submit images of medicinal plants, and receive accurate identification results, thanks to the powerful machine learning model hosted in the Flask application. The Google Cloud hosting further enhances reliability', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='further enhances reliability and scalability, making it an effective platform for addressing the complex challenges of medicinal plant identification in the context of Ayurvedic Pharmaceutics.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Phase 4: Testing ( Ongoing )', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='We have started Testing and continuously made improvements by revisiting both the Data collection and Model Building phases, we are undergoing planning and executing a variety of testing methods to make our platform more robust and reliable. We are also', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='and reliable. We are also looking to conduct a user feedback survey and incorporate user needs with our objectives.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Features of our Solution :', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='1. Standalone Android Application: The solution is delivered as a standalone Android application, ensuring that it can operate without a constant internet connection. This feature is especially valuable in areas with limited or unreliable network', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='limited or unreliable network connectivity, such as rural and remote locations. Users can access and utilize the application regardless of network availability, addressing a crucial need in these regions.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='2. Offloading Computation to the Cloud: The system efficiently offloads computational tasks to the cloud via a web application. This approach minimizes the computational load on the client device, enabling the application to run smoothly even on low-end', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"to run smoothly even on low-end hardware, such as feature phones. This not only ensures accessibility for users with limited resources but also enhances the application's responsiveness.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='3. Two-Tier Architecture for Microservices: The two-tier architecture employed allows for the development of independent applications that can interact with the machine learning model as a microservice through an API. This architectural design promotes', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"architectural design promotes flexibility and scalability, making it convenient to build various applications and services that leverage the model's capabilities.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='4. Feedback Mechanism: The system incorporates a feedback mechanism, which facilitates the continuous integration of new data points into the model. This feature ensures that the model remains up-to-date and relevant by incorporating new information and', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='new information and insights over time. Users can contribute to the improvement of the system, making it more accurate and adaptable.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"5. Custom Knowledge Database: To enhance user experience and reliability, the solution relies on a manually collected custom knowledge database. This database provides users with accurate and trustworthy descriptions related to the model's predictions.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"to the model's predictions. This supplementary information offers users valuable context and understanding of the results, enhancing the overall utility and reliability of the application.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Model Development:\\n\\n\\t\\t1. Keras with ResNet-50: We utilize Keras with the ResNet-50 architecture for model development. ResNet-50 is a powerful pre-trained deep learning model that enables us to achieve accurate image recognition.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='2. PyTorch: For image preprocessing tasks, such as data augmentation, resizing, and data transformations, we leverage PyTorch, a versatile Python library that provides the flexibility we need.\\n\\nAndroid Application Development:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"3. Java: We've built our Android application using Java, a widely used programming language for Android app development.\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='4. TensorFlow Lite (TFLite): Our Android app integrates TensorFlow Lite (TFLite) models, ensuring that users can enjoy real-time image recognition on their mobile devices with efficiency and speed.\\n\\n\\tWeb Application Development:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Web Application Development:\\n\\n\\t\\t5. HTML, CSS, JavaScript: HTML forms the backbone of our web application, structuring web pages, while CSS is responsible for styling, and JavaScript adds interactivity, enhancing the overall user experience.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='6. Flask (Python Web Framework): Flask is our choice for the back-end development of our web application. It handles API requests, server-side logic, and interactions with our machine-learning model with ease.\\n\\n\\tCloud Services and Deployment:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='7. Google Cloud Platform (GCP): GCP plays a pivotal role in hosting and deploying our platform. We rely on services like Google App Engine Platform for scalability and performance.\\n\\n\\tDatabase and Data Management:', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='Database and Data Management:\\n\\n\\t\\t8. MySQL: For data storage, we use MySQL, a robust relational database management system that ensures data integrity and reliable data management.', metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content=\"In closing, our commitment is to provide a practical and impactful solution. Our platform aims to empower individuals with accurate medicinal material recognition and essential traditional medicinal knowledge. We're bridging the gap between traditional\", metadata={'source': 'Idea Description.txt'}),\n",
              " Document(page_content='the gap between traditional wisdom and modern technology, ensuring that medicinal material information is readily accessible to laymen and professionals alike.', metadata={'source': 'Idea Description.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try :\n",
        "  docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "  print('existing')\n",
        "except:\n",
        "  docsearch = Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)\n",
        "  print('new')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxYCJFuy8OdS",
        "outputId": "33b4c053-d482-4208-f007-4b14d615be99"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "existing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Frontend'\n",
        "docs = docsearch.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4VxrxWm6DYG",
        "outputId": "63015837-f327-4094-cc06-6ce93aa5f484"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Front-End Component:'),\n",
              " Document(page_content='The front-end component is the user-facing part of the platform, responsible for interacting with users and facilitating the input of data, particularly images in this context. Users typically access the front end through a user interface, which may be'),\n",
              " Document(page_content='Back-End Component:'),\n",
              " Document(page_content='3. Interaction between components: The back-end component serves as a bridge between the front-end and the machine learning model. It communicates with the front end to receive user inputs and, once the inference is complete, sends the results back to')]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager\n",
        "model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\n",
        "model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "31bde098c973405e96d3f510de787dad",
            "0dac7cf8902049eba8c35964ca7d8c64",
            "339c39d6590643bdaea1022580fb9c23",
            "9b389355cbbe45e7947920c8462fb39a",
            "132cdd933b0847749e11c909904efbdd",
            "f05a0a0d7cbc4837b6f387ff872ea81d",
            "08b5fbe958d643e081ad42d2d2e08006",
            "0c44fe1516e747b0ba68eeb862de8523",
            "8524be071c484f0fb223cb642c890766",
            "cbb310ddb8844e9f8374c07b9561df8c",
            "8c3c9e002ebd4a82ab75f8307d06c02e"
          ]
        },
        "id": "US8YBIym9COb",
        "outputId": "9c551ec6-0919-4331-f728-0b696abcdd73"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b-python.Q5_K_M.gguf:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31bde098c973405e96d3f510de787dad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 32\n",
        "n_batch = 512\n",
        "llm = LlamaCpp(\n",
        "              model_path=model_path,\n",
        "              max_tokens=256,\n",
        "              n_gpu_layers=n_gpu_layers,\n",
        "              n_batch=n_batch,\n",
        "              callback_manager=callback_manager,\n",
        "              n_ctx=1024,\n",
        "              verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06df4aab-8e95-4342-8cae-4f90f3855e58",
        "id": "eT2oT8V99COc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "osx4oPFW9COd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is the problem statement'\n",
        "docs = docsearch.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBHGvt4D9Toj",
        "outputId": "c8422417-dea7-476a-8aa0-bed5eb90b107"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Features of our Solution :'),\n",
              " Document(page_content='Features of our Solution :'),\n",
              " Document(page_content='Features of our Solution :'),\n",
              " Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "Uk_N6HEvB-IB"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":1, \"max_length\":64}, huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8593e281-3d74-4ff5-9221-01fc3b5a955a",
        "id": "XUnmLezvB-IC"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "3PiltNdeB-IC"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what are the vaious tech stack used in detail ?\"\n",
        "docs=docsearch.similarity_search(query)\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pevG3FceB-ID",
        "outputId": "b538a077-9dbc-4569-a73e-06c95aa79247"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:'), Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:'), Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:'), Document(page_content='for deployment, scaling, and management, ensuring that the platform is robust and responsive.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chain.run(input_documents=docs, question=query)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3c34925a-4642-4378-febe-050e175de1b7",
        "id": "cxxf29lDB-ID"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for deployment, scaling, and management, ensuring that the platform is robust and responsive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RetrieverQA**"
      ],
      "metadata": {
        "id": "brfFelJHqs2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer :\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)"
      ],
      "metadata": {
        "id": "fP5jK9Oip8b9"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the architecture in very detail\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "KyEPSttosO24",
        "outputId": "b8e64962-ed13-477c-a7ac-01fa3bb09273"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This two-tier architecture neatly separates the user interaction and data collection (front-end)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A (not so) Elegant Function**"
      ],
      "metadata": {
        "id": "s-r65Gr_hJhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pinecone-client\n",
        "# !pip install huggingface-hub\n",
        "# !pip install sentence-transformers\n",
        "# !pip install langchain\n",
        "# !pip install pypdf\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "i9O-W2d8ihgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pinecone\n",
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize Pinecone\n",
        "HUGGINGFACEHUB_API_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_API_ENV = userdata.get('PINECONE_API_ENV')\n",
        "\n",
        "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
        "index_name = \"bot\"  # Choose a name for your Pinecone index\n",
        "\n",
        "# Define functions to clean up the code\n",
        "def setup_embeddings():\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "    return embeddings\n",
        "\n",
        "def setup_pdf_loader(file_path):\n",
        "    # Initialize PDF loader\n",
        "    return PyPDFLoader(file_path)\n",
        "\n",
        "def split_text_into_chunks(data, chunk_size=500, chunk_overlap=0):\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs = text_splitter.split_documents(data)\n",
        "    return docs\n",
        "\n",
        "def initialize_pinecone_index(embeddings, docs, index_name):\n",
        "    try:\n",
        "        # If the index already exists, load it\n",
        "        docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "    except:\n",
        "        # Otherwise, create a new index\n",
        "        docsearch = Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)\n",
        "    return docsearch\n",
        "\n",
        "def setup_llama_cpp(model_name_or_path, callback_manager, max_tokens=256, n_gpu_layers=32, n_batch=512, n_ctx=1024):\n",
        "    # Download and set up LlamaCpp\n",
        "    model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n",
        "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "    llm = LlamaCpp(\n",
        "        model_path=model_path,\n",
        "        max_tokens=max_tokens,\n",
        "        n_gpu_layers=n_gpu_layers,\n",
        "        n_batch=n_batch,\n",
        "        callback_manager=callback_manager,\n",
        "        n_ctx=n_ctx,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "def load_qa_model(llm, chain_type=\"stuff\"):\n",
        "    # Load question-answering chain\n",
        "    return load_qa_chain(llm, chain_type=chain_type)\n",
        "\n",
        "def query_documents(docsearch, query):\n",
        "    # Perform a similarity search\n",
        "    return docsearch.similarity_search(query)\n",
        "\n",
        "def determine_file_type(file_path):\n",
        "    if file_path.lower().endswith('.pdf'):\n",
        "        return 'pdf'\n",
        "    elif file_path.lower().endswith('.txt'):\n",
        "        return 'txt'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Only PDF and TXT files are supported.\")\n",
        "\n",
        "# Main code\n",
        "def main():\n",
        "    # Provide the path to the file (PDF or TXT)\n",
        "    file_path = \"/content/Idea Description.txt\"\n",
        "\n",
        "    # Determine the file type\n",
        "    file_type = determine_file_type(file_path)\n",
        "\n",
        "    if file_type == 'pdf':\n",
        "        # Step 1: Initialize PDF loader\n",
        "        pdf_loader = setup_pdf_loader(file_path)\n",
        "\n",
        "        # Step 2: Load the PDF data\n",
        "        data = pdf_loader.load()\n",
        "\n",
        "    elif file_type == 'txt':\n",
        "        # Read the text data from the TXT file\n",
        "        loader = TextLoader(\"Idea Description.txt\")\n",
        "        data = loader.load()\n",
        "\n",
        "\n",
        "    # Step 3: Split the text into chunks\n",
        "    docs = split_text_into_chunks(data)\n",
        "    print(docs)\n",
        "\n",
        "    # Step 4: Initialize embeddings\n",
        "    embeddings = setup_embeddings()\n",
        "\n",
        "    # Step 5: Initialize Pinecone index\n",
        "    docsearch = initialize_pinecone_index(embeddings, docs, index_name)\n",
        "\n",
        "    # Step 6: Set up LlamaCpp\n",
        "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "    # llm = setup_llama_cpp(\n",
        "    #     model_name_or_path=\"TheBloke/CodeLlama-13B-Python-GGUF\",\n",
        "    #     callback_manager=callback_manager,\n",
        "    # )\n",
        "\n",
        "    # using huggingface\n",
        "    llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512}, huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "    # Step 7: Load the question-answering model\n",
        "    chain = load_qa_model(llm, chain_type=\"stuff\")\n",
        "\n",
        "    # Step 8: Perform a similarity search\n",
        "    query = \"What are the various technologies used ?\"\n",
        "    docs = query_documents(docsearch, query)\n",
        "\n",
        "    # Step 9: Run question answering on the retrieved documents\n",
        "    print(chain.run(input_documents=docs, question=query))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AHurpyehJHv",
        "outputId": "9c7d8c8c-cb0e-494e-dfc1-02df9d1ec5af"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Problem Statement: Identification of Different Medicinal Plants/Raw materials through Image Processing Using Machine Learning Algorithms\\n\\n\\nSolutions :\\n\\nA user-friendly Web-based and Android-based platform for users to upload images, find out whether it’s a medical plant/material or not, and provide detailed descriptions of the benefits of the recognized leaf/material using deep learning techniques.', metadata={'source': 'Idea Description.txt'}), Document(page_content='Our Idea/Solution is executed in 4 major phases, viz. Data Collection and Processing, Model Building, Platform Integration and Rigorous Testing. Lets look at them comprehensively :\\n\\nPhase 1: Dataset Collection', metadata={'source': 'Idea Description.txt'}), Document(page_content='We have used a publicly available dataset as a starter for the problem, and plan to make this dataset more robust by adding more data points.\\n\\t\\n\\tCurrent Dataset Description:\\n\\t\\tThe dataset consists of images of 30 species of healthy medicinal herbs, including Santalum album (Sandalwood), Muntingia calabura (Jamaica cherry), Plectranthus amboinicus / Coleus amboinicus (Indian Mint, Mexican mint), Brassica juncea (Oriental mustard), and more.', metadata={'source': 'Idea Description.txt'}), Document(page_content='In total, there are 1,500 images encompassing 30 species, with each species having 60 to 100 high-quality images.\\n\\t\\n\\tDataset: S, Roopashree; J, Anitha (2020), “Medicinal Leaf Dataset”, Mendeley Data, V1, doi: 10.17632/nnytj2v3n5.1', metadata={'source': 'Idea Description.txt'}), Document(page_content='Phase 2: Building a Model\\n\\tTo solve the problem of identification we employ transfer learning techniques to use state-of-the-art CNN models like ResNet50 and make them fit for our use-case.', metadata={'source': 'Idea Description.txt'}), Document(page_content='Transfer learning is a machine learning technique that leverages the knowledge acquired by a model in one task to improve its performance in another related task. In this case, a pre-trained ResNet-50 model, a deep convolutional neural network known for its image classification capabilities, serves as the foundation. This pre-trained model has already learned to recognise and extract valuable features from a vast and diverse dataset.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"We employ two key strategies :\\n\\n\\t1. Feature Extraction: The first strategy is feature extraction. This entails preserving the knowledge and feature extraction abilities of the pre-trained ResNet model. To achieve this, we freeze the model's weights, ensuring that the original features learned during its extensive training are not overwritten. This is vital because these pre-learned features are valuable for image recognition tasks, including medicinal materials/plant classification.\", metadata={'source': 'Idea Description.txt'}), Document(page_content=\"2. Custom Classifier: The second strategy involves customising the model for the specific task of medicinal plant/material identification. Instead of using the pre-trained model's original classification layer, a new classifier is added to the model. This classifier is tailored to the nuances of the medicinal plant/material classification problem and consists of fully connected layers, activation functions, and dropout layers to prevent overfitting.\", metadata={'source': 'Idea Description.txt'}), Document(page_content='In the training process, we monitoring both training and validation losses and accuracy. We implement early stopping, a technique that halts training when there is no improvement in the validation loss for a predefined number of epochs(rounds). This prevents the model from overfitting the training data and ensures that it generalizes well to new, unseen images.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"As the training progresses, we continuously evaluate the model's performance on a separate validation dataset. The best-performing model, based on validation loss, is saved. This means that the model with the lowest validation loss is retained for our future use.\", metadata={'source': 'Idea Description.txt'}), Document(page_content='The result is a highly accurate model for identifying medicinal plants, with an astounding testing accuracy of 98%. This impressive level of accuracy shows us the effectiveness and potential of the transfer learning approach in solving the challenging problem of medicinal plant/material identification, ultimately contributing to the trustworthiness and authenticity of the Ayurvedic system.\\n\\n\\nPhase 2: Creating a platform (we are Continuously Enhancing on our platform)', metadata={'source': 'Idea Description.txt'}), Document(page_content='Front-End Component:\\n\\t\\tThe front-end component is the user-facing part of the platform, responsible for interacting with users and facilitating the input of data, particularly images in this context. Users typically access the front end through a user interface, which may be a web application, mobile app, or other user-friendly interface. This component collects user inputs, such as images of medicinal plants, and communicates with the back end to process and analyze this data.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"Back-End Component:\\n\\t\\tOur back-end component, hosted on Google Cloud, plays a significant role in the platform's operation. It is implemented as a Flask application, which is a lightweight and versatile web framework for Python. The back end serves as the engine that performs several critical functions:\", metadata={'source': 'Idea Description.txt'}), Document(page_content='1. Input Image Preprocessing: When users submit images of medicinal plants, the back end handles the preprocessing of these images. This preprocessing may involve tasks like resizing, normalizing, and ensuring that the input data is in a suitable format for the model. Preprocessing is a crucial step to ensure that the model receives clean and consistent input data.', metadata={'source': 'Idea Description.txt'}), Document(page_content='2. Inferencing with the Model: The heart of the back-end is the previously described model, which has been fine-tuned and optimized for medicinal plant identification. The back end takes the preprocessed images and passes them through the model for inference. The model uses its learned features to classify the medicinal plants accurately. The result of this inference is returned to the front end.', metadata={'source': 'Idea Description.txt'}), Document(page_content='3. Interaction between components: The back-end component serves as a bridge between the front-end and the machine learning model. It communicates with the front end to receive user inputs and, once the inference is complete, sends the results back to the front end for display to the user.', metadata={'source': 'Idea Description.txt'}), Document(page_content='4. Scalability and Cloud Hosting: Hosting the back-end on Google Cloud provides scalability, allowing the platform to handle increased user loads efficiently. Google Cloud offers a range of services that can be leveraged for deployment, scaling, and management, ensuring that the platform is robust and responsive.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"This two-tier architecture neatly separates the user interaction and data collection (front-end) from the model's inference process (back-end), creating a modular and scalable system. It ensures that users can easily and intuitively interact with the platform, submit images of medicinal plants, and receive accurate identification results, thanks to the powerful machine learning model hosted in the Flask application. The Google Cloud hosting further enhances reliability and scalability, making\", metadata={'source': 'Idea Description.txt'}), Document(page_content='it an effective platform for addressing the complex challenges of medicinal plant identification in the context of Ayurvedic Pharmaceutics.', metadata={'source': 'Idea Description.txt'}), Document(page_content='Phase 4: Testing ( Ongoing )\\n\\tWe have started Testing and continuously made improvements by revisiting both the Data collection and Model Building phases, we are undergoing planning and executing a variety of testing methods to make our platform more robust and reliable. We are also looking to conduct a user feedback survey and incorporate user needs with our objectives.', metadata={'source': 'Idea Description.txt'}), Document(page_content='Features of our Solution :\\n\\t1. Standalone Android Application: The solution is delivered as a standalone Android application, ensuring that it can operate without a constant internet connection. This feature is especially valuable in areas with limited or unreliable network connectivity, such as rural and remote locations. Users can access and utilize the application regardless of network availability, addressing a crucial need in these regions.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"2. Offloading Computation to the Cloud: The system efficiently offloads computational tasks to the cloud via a web application. This approach minimizes the computational load on the client device, enabling the application to run smoothly even on low-end hardware, such as feature phones. This not only ensures accessibility for users with limited resources but also enhances the application's responsiveness.\", metadata={'source': 'Idea Description.txt'}), Document(page_content=\"3. Two-Tier Architecture for Microservices: The two-tier architecture employed allows for the development of independent applications that can interact with the machine learning model as a microservice through an API. This architectural design promotes flexibility and scalability, making it convenient to build various applications and services that leverage the model's capabilities.\", metadata={'source': 'Idea Description.txt'}), Document(page_content='4. Feedback Mechanism: The system incorporates a feedback mechanism, which facilitates the continuous integration of new data points into the model. This feature ensures that the model remains up-to-date and relevant by incorporating new information and insights over time. Users can contribute to the improvement of the system, making it more accurate and adaptable.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"5. Custom Knowledge Database: To enhance user experience and reliability, the solution relies on a manually collected custom knowledge database. This database provides users with accurate and trustworthy descriptions related to the model's predictions. This supplementary information offers users valuable context and understanding of the results, enhancing the overall utility and reliability of the application.\", metadata={'source': 'Idea Description.txt'}), Document(page_content='Tech Stack Description : \\n\\tWe have used a diverse tech stack to address the problem statement. Tech-Stack for different components are as f\\n\\n\\tModel Development:\\n\\n\\t\\t1. Keras with ResNet-50: We utilize Keras with the ResNet-50 architecture for model development. ResNet-50 is a powerful pre-trained deep learning model that enables us to achieve accurate image recognition.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"2. PyTorch: For image preprocessing tasks, such as data augmentation, resizing, and data transformations, we leverage PyTorch, a versatile Python library that provides the flexibility we need.\\n\\nAndroid Application Development:\\n\\n\\t\\t3. Java: We've built our Android application using Java, a widely used programming language for Android app development.\", metadata={'source': 'Idea Description.txt'}), Document(page_content='4. TensorFlow Lite (TFLite): Our Android app integrates TensorFlow Lite (TFLite) models, ensuring that users can enjoy real-time image recognition on their mobile devices with efficiency and speed.\\n\\n\\tWeb Application Development:\\n\\n\\t\\t5. HTML, CSS, JavaScript: HTML forms the backbone of our web application, structuring web pages, while CSS is responsible for styling, and JavaScript adds interactivity, enhancing the overall user experience.', metadata={'source': 'Idea Description.txt'}), Document(page_content='6. Flask (Python Web Framework): Flask is our choice for the back-end development of our web application. It handles API requests, server-side logic, and interactions with our machine-learning model with ease.\\n\\n\\tCloud Services and Deployment:\\n\\n\\t\\t7. Google Cloud Platform (GCP): GCP plays a pivotal role in hosting and deploying our platform. We rely on services like Google App Engine Platform for scalability and performance.\\n\\n\\tDatabase and Data Management:', metadata={'source': 'Idea Description.txt'}), Document(page_content='8. MySQL: For data storage, we use MySQL, a robust relational database management system that ensures data integrity and reliable data management.', metadata={'source': 'Idea Description.txt'}), Document(page_content=\"In closing, our commitment is to provide a practical and impactful solution. Our platform aims to empower individuals with accurate medicinal material recognition and essential traditional medicinal knowledge. We're bridging the gap between traditional wisdom and modern technology, ensuring that medicinal material information is readily accessible to laymen and professionals alike.\", metadata={'source': 'Idea Description.txt'})]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a user interface, which may be a web application, mobile app, or other user-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xl92HxIssF-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
